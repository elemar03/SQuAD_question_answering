{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models for the OWQA task\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    ">The next cell contains functions from the official evaluation script for the SQuAD dataset 1.1.\n",
    ">Taken from:\n",
    ">https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ START ##############################################\n",
    "##### Taken from: https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py #####\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn') \n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(lower(s))\n",
    "\n",
    "\n",
    "def topn_score(predictions, ground_truth):\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if (normalize_answer(pred) == normalize_answer(ground_truth)):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def em_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "##### Taken from: https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py #####\n",
    "############################################# END ##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Functions\n",
    "----\n",
    "\n",
    "\n",
    "### Function to evaluate the model performance on F1 and EM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    top2 = top3 = em = 0\n",
    "    total = len(data['start_wordloc'])\n",
    "\n",
    "    ground_truths = [np.argmax(data['start_wordloc'][i]) for i in range(total)]\n",
    "    preds = [data['start_pred'][i].argsort()[-1] for i in range(total)]\n",
    "    preds2 = [data['start_pred'][i].argsort()[-2] for i in range(total)]\n",
    "    preds3 = [data['start_pred'][i].argsort()[-3] for i in range(total)]\n",
    "    \n",
    "    for i in range(total):\n",
    "        context = data['contexts_tokens'][i]\n",
    "        ground_truth = context[ground_truths[i]]\n",
    "        #print(ground_truth, data['answers'][i])\n",
    "        \n",
    "        # if the answer is within the length of its specific context\n",
    "        if preds[i] < len(context) and preds2[i] < len(data['contexts_tokens'][i]) and preds3[i] < len(context):\n",
    "            prediction = context[preds[i]]\n",
    "            prediction2 = context[preds2[i]]\n",
    "            prediction3 = context[preds3[i]]\n",
    "            \n",
    "            em += em_score(prediction, ground_truth)\n",
    "            top2 += topn_score([prediction, prediction2], ground_truth)\n",
    "            top3 += topn_score([prediction, prediction2, prediction3], ground_truth)\n",
    "\n",
    "    em = 100.0 * em / total\n",
    "    top2 = 100.0 * top2 / total\n",
    "    top3 = 100.0 * top3 / total\n",
    "\n",
    "    return {'em': em, 'top2': top2, 'top3': top3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval(model, evaluation):\n",
    "    for name, val in zip(model.metrics_names, evaluation):\n",
    "        print(' '*25, name, ' = ', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Main Function called in the models\n",
    "\n",
    "Pretty prints all the model results and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_main(train_data, dev_data, model, history_dict):\n",
    "    train_evaluation = model.evaluate([train_data['contexts'], train_data['questions']],\n",
    "                    train_data['start_wordloc'], verbose=1)\n",
    "\n",
    "    dev_evaluation = model.evaluate([dev_data['contexts'], dev_data['questions']],\n",
    "                    dev_data['start_wordloc'], verbose=1)\n",
    "    \n",
    "    train_scores = evaluate(train_data)\n",
    "    dev_scores = evaluate(dev_data)\n",
    "    \n",
    "    print('\\n'+'_'*110, '\\n')\n",
    "    print(' '*37, 'TRAINING LOSS AND ACCURACY PLOTS')\n",
    "    print('_'*110, '\\n')\n",
    "    plot(history_dict)\n",
    "    \n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*40, 'EVALUATION ON TRAINING SET')\n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*12, 'EM: ', train_scores['em'], ';   Top2-EM: ', train_scores['top2'],\n",
    "                                              ';   Top3-EM: ', train_scores['top3'], '\\n')\n",
    "    print_eval(model, train_evaluation)\n",
    "    \n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*39, 'EVALUATION ON DEVELOPMENT SET')\n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*12, 'EM: ', dev_scores['em'], ';   Top2-EM: ', dev_scores['top2'], \n",
    "                                            ';   Top3-EM: ', dev_scores['top3'], '\\n')\n",
    "    print_eval(model, dev_evaluation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Printing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(output):\n",
    "    \n",
    "    x = range(1, len(output) + 1)\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(x, output)\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Softmax Output')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Plotting One-word Answers Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history_dict):\n",
    "    \n",
    "    train_loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    \n",
    "    eps = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.plot(eps, train_loss, 'coral', label = 'Training loss')\n",
    "    plt.plot(eps, val_loss, 'purple', label = 'Validation loss')\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(history_dict):\n",
    "    \n",
    "    train_acc = history_dict['acc']\n",
    "    val_acc = history_dict['val_acc']\n",
    "    \n",
    "    eps = range(1, len(train_acc) + 1)\n",
    "\n",
    "    plt.plot(eps, train_acc, 'coral', label = 'Training acc')\n",
    "    plt.plot(eps, val_acc, 'purple', label = 'Validation acc')\n",
    "    plt.title('Training and validation acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
