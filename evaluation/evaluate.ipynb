{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models for the QA task\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    ">The next cell contains functions from the official evaluation script for the SQuAD dataset 1.1.\n",
    ">Taken from:\n",
    ">https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################ START ##############################################\n",
    "##### Taken from: https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py #####\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn') \n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def em_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "##### Taken from: https://github.com/abisee/cs224n-win18-squad/blob/master/code/evaluate.py #####\n",
    "############################################# END ##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Evaluation Functions\n",
    "----\n",
    "\n",
    "\n",
    "### Function to evaluate the model performance on F1 and EM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    f1 = em = 0\n",
    "    total = len(data['answers'])\n",
    "    \n",
    "    start_ground_truths = [np.argmax(data['start_wordloc'][i]) for i in range(total)]\n",
    "    end_ground_truths = [np.argmax(data['end_wordloc'][i]) for i in range(total)]\n",
    "    start_preds = [np.argmax(data['start_pred'][i]) for i in range(total)]\n",
    "    end_preds = [np.argmax(data['end_pred'][i]) for i in range(total)]\n",
    "    \n",
    "    for i in range(total):\n",
    "        ground_truth = data['contexts_tokens'][i][start_ground_truths[i]:end_ground_truths[i]]\n",
    "        ground_truth = \" \".join(ground_truth)\n",
    "        #print(ground_truth, data['answers'][i])\n",
    "        \n",
    "        # if the answer is within the length of its specific context\n",
    "        if start_preds[i] < len(data['contexts_tokens'][i]) or end_preds[i] < len(data['contexts_tokens'][i]):\n",
    "            prediction = data['contexts_tokens'][i][start_preds[i]:end_preds[i]]     \n",
    "            prediction = \" \".join(prediction)\n",
    "            em += em_score(prediction, ground_truth)\n",
    "            f1 += f1_score(prediction, ground_truth)\n",
    "\n",
    "    em = 100.0 * em / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'em': em, 'f1': f1}\n",
    "\n",
    "def print_eval(model, evaluation):\n",
    "    for name, val in zip(model.metrics_names, evaluation):\n",
    "        print(' '*25, name, ' = ', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Main Function called in the models\n",
    "\n",
    "Pretty prints all the model results and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_main(train_data, dev_data, model, history_dict):\n",
    "    train_evaluation = model.evaluate([train_data['contexts'], train_data['questions']],\n",
    "                    [train_data['start_wordloc'], train_data['end_wordloc']], verbose=1)\n",
    "\n",
    "    dev_evaluation = model.evaluate([dev_data['contexts'], dev_data['questions']],\n",
    "                    [dev_data['start_wordloc'], dev_data['end_wordloc']], verbose=1)\n",
    "    \n",
    "    train_scores = evaluate(train_data)\n",
    "    dev_scores = evaluate(dev_data)\n",
    "    \n",
    "    print('\\n'+'_'*110, '\\n')\n",
    "    print(' '*37, 'TRAINING LOSS AND ACCURACY PLOTS')\n",
    "    print('_'*110, '\\n')\n",
    "    plot_multioutput(history_dict)\n",
    "    \n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*40, 'EVALUATION ON TRAINING SET')\n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*25, 'EM: ', train_scores['em'], ';     F1: ', train_scores['f1'], '\\n')\n",
    "    print_eval(model, train_evaluation)\n",
    "    \n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*39, 'EVALUATION ON DEVELOPMENT SET')\n",
    "    print('_'*110, '\\n')\n",
    "    print(' '*25, 'EM: ', dev_scores['em'], ';     F1: ', dev_scores['f1'], '\\n')\n",
    "    print_eval(model, dev_evaluation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Printing the Softmax Activation of the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(output):\n",
    "    \n",
    "    x = range(1, len(output) + 1)\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(x, output)\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Softmax Output')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Plotting Multi-Output Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multioutput(history_dict):\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(5) # optional setting the height of the image\n",
    "    fig.set_figwidth(16)\n",
    "    \n",
    "    plot_loss, plot_acc = fig.add_subplot(1,2,1), fig.add_subplot(1,2,2)\n",
    "    train_loss = history_dict['loss']\n",
    "    eps = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    start_loss = history_dict['start_loss']\n",
    "    end_loss = history_dict['end_loss']\n",
    "    val_start_loss = history_dict['val_start_loss']\n",
    "    val_end_loss = history_dict['val_end_loss']\n",
    "    \n",
    "    start_acc = history_dict['start_acc']\n",
    "    end_acc = history_dict['end_acc']\n",
    "    val_start_acc = history_dict['val_start_acc']\n",
    "    val_end_acc = history_dict['val_end_acc']\n",
    "    \n",
    "    plot_loss.plot(eps, start_loss, 'deepskyblue', label = 'Start Training loss')\n",
    "    plot_loss.plot(eps, end_loss, 'dodgerblue', label = 'End Training loss')\n",
    "    plot_loss.plot(eps, val_start_loss, 'mediumslateblue', label = 'Start Validation loss')\n",
    "    plot_loss.plot(eps, val_end_loss, 'blueviolet', label = 'End Validation loss')\n",
    "    \n",
    "    plot_acc.plot(eps, start_acc, 'deepskyblue', label = 'Start Training acc')\n",
    "    plot_acc.plot(eps, end_acc, 'dodgerblue', label = 'End Training acc')\n",
    "    plot_acc.plot(eps, val_start_acc, 'mediumslateblue', label = 'Start Validation acc')\n",
    "    plot_acc.plot(eps, val_end_acc, 'blueviolet', label = 'End Validation acc')\n",
    "    \n",
    "    plot_loss.set_title(\"Training and validation loss\")\n",
    "    plot_loss.set_xlabel('Epochs')\n",
    "    plot_loss.set_ylabel('Loss')\n",
    "    plot_acc.set_title(\"Training and validation accuracy\")\n",
    "    plot_acc.set_xlabel('Epochs')\n",
    "    plot_acc.set_ylabel('Accuracy')\n",
    "    plot_loss.legend(loc=\"upper left\")\n",
    "    plot_acc.legend(loc=\"upper left\")\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
