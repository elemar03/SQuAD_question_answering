{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Considering only the contexts smaller than  1400  . . .\n",
      "Number of span align problems:  14\n",
      "Train data length = 84265\n",
      "** Considering only the contexts smaller than  1400  . . .\n",
      "Number of span align problems:  0\n",
      "Test data length = 10102\n",
      "\n",
      " - train -\n",
      "** Tokenizing contexts and questions. . .\n",
      "** Mapping character location to word location of the answer . . .\n",
      "Not successful char loc -> word loc mapping:  2437\n",
      "** Discarding 2437 entries due to char loc -> word loc mapping problems . . .\n",
      "Dataset length before:  84265\n",
      "Dataset length after:  81828\n",
      "Not successful char loc -> word loc mapping:  0\n",
      "Number of not one-word answers:  55433\n",
      "** Filtering One-Word Answers only. . .\n",
      "Dataset length before:  81828\n",
      "Dataset length after:  26395\n",
      "Number of not one-word answers:  0\n",
      "\n",
      " - dev -\n",
      "** Tokenizing contexts and questions. . .\n",
      "** Mapping character location to word location of the answer . . .\n",
      "Not successful char loc -> word loc mapping:  322\n",
      "** Discarding 322 entries due to char loc -> word loc mapping problems . . .\n",
      "Dataset length before:  10102\n",
      "Dataset length after:  9780\n",
      "Not successful char loc -> word loc mapping:  0\n",
      "Number of not one-word answers:  6778\n",
      "** Filtering One-Word Answers only. . .\n",
      "Dataset length before:  9780\n",
      "Dataset length after:  3002\n",
      "Number of not one-word answers:  0\n",
      "Vocabulary size:  85837\n",
      "** Converting the tokens into sequences . . .\n",
      " -- Ends max length train/dev:  275 235\n",
      " -- Contexts max length:  276\n",
      " -- Questions max length:  36\n",
      "** Padding the sequences . . .\n",
      "** Writing the preprocessed data in the respective files. . .\n",
      "** Writing the preprocessed data in the respective files. . .\n",
      "** One-hot encoding the labels in two tensors . . .\n",
      "** One-hot encoding the labels in two tensors . . .\n",
      "(26395, 276)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "%run ../../preprocessing/ow_preprocessing.ipynb\n",
    "\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input, Dropout, Embedding, Flatten, Dense, Bidirectional, LSTM, GRU\n",
    "from keras.layers import RepeatVector, Activation, merge, Lambda, Flatten, Reshape, TimeDistributed\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "#squad_preprocessing_main(contexts_max_len=1400, max_size=None)\n",
    "vocabulary_size = 85838\n",
    "\n",
    "# Get the preprocessed data from the files\n",
    "train_data = get_data_from_file(\"train\")\n",
    "dev_data = get_data_from_file(\"dev\")\n",
    "\n",
    "# Calculate the maximum length of the contexts and the questions\n",
    "assert max_len(train_data['contexts']) == max_len(dev_data['contexts'])\n",
    "assert max_len(train_data['questions']) == max_len(dev_data['questions'])\n",
    "c_maxlen =  max_len(train_data['contexts'])\n",
    "q_maxlen =  max_len(train_data['questions'])\n",
    "\n",
    "# Encode the labels\n",
    "train_data['start_wordloc'], _ = onehot_labels(train_data['start_wordloc'], train_data['end_wordloc'], c_maxlen)\n",
    "dev_data['start_wordloc'], _ = onehot_labels(dev_data['start_wordloc'], dev_data['end_wordloc'], c_maxlen)\n",
    "print(train_data['contexts'].shape)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
