{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layers\n",
    "\n",
    "---\n",
    "Taken from: https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras\n",
    "\n",
    "----\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2017 Wentao Zhu\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical#, accuracy\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed, Recurrent\n",
    "\n",
    "def time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, units=None, timesteps=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        units: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not units:\n",
    "        units = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b:\n",
    "        x += b\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, units]))\n",
    "        x.set_shape([None, None, units])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, units))\n",
    "    return x\n",
    "\n",
    "class Attention(Recurrent):\n",
    "    \"\"\"Attention Recurrent Unit - Bengio et al. ICLR 2015.\n",
    "    # Arguments\n",
    "        units: dimension of the internal projections and the final output.\n",
    "        h: we use it as attention to process the input\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, h, h_dim,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                 #activation='tanh', inner_activation='hard_sigmoid',\n",
    "                 #W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                 #dropout_W=0., dropout_U=0., \n",
    "                 **kwargs):\n",
    "        self.units = units\n",
    "        self.h = h[:,-1,:]\n",
    "        self.h_dim = h_dim\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        #self.activation = activations.get(activation)\n",
    "        #self.inner_activation = activations.get(inner_activation)\n",
    "        #self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        #self.U_regularizer = regularizers.get(U_regularizer)\n",
    "        #self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        #self.dropout_W = dropout_W\n",
    "        #self.dropout_U = dropout_U\n",
    "\n",
    "        #if self.dropout_W or self.dropout_U:\n",
    "        #    self.uses_learning_phase = True\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.input_dim = input_shape[2]\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: all-zero tensor of shape (units)\n",
    "            self.states = [None]\n",
    "\n",
    "        self.Wa = self.add_weight((self.units, self.units),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Wa'.format(self.name))\n",
    "        self.Ua = self.add_weight((self.h_dim, self.units),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Ua'.format(self.name))\n",
    "        self.Va = self.add_weight((self.units,1),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Va'.format(self.name))\n",
    "        self.Wzr = self.add_weight((self.input_dim, 2 * self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Uzr = self.add_weight((self.units, 2 * self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Czr = self.add_weight((self.h_dim, 2 * self.units),\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   name='{}_Czr'.format(self.name))\n",
    "        self.W = self.add_weight((self.input_dim, self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        self.U = self.add_weight((self.units, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_U'.format(self.name))\n",
    "        self.C = self.add_weight((self.h_dim, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_C'.format(self.name))\n",
    "        \n",
    "        #if self.initial_weights is not None:\n",
    "        #    self.set_weights(self.initial_weights)\n",
    "        #    del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        if not input_shape[0]:\n",
    "            raise ValueError('If a RNN is stateful, a complete '\n",
    "                             'input_shape must be provided '\n",
    "                             '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.units)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.units))]\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        #if self.consume_less == 'cpu':\n",
    "        #    input_shape = K.int_shape(x)\n",
    "        #    input_dim = input_shape[2]\n",
    "        #    timesteps = input_shape[1]\n",
    "\n",
    "        #    x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    return K.concatenate([x_z, x_r, x_h], axis=2)\n",
    "        #else:\n",
    "        #    return x\n",
    "        self.ha = time_distributed_dense(self.h, self.Ua)\n",
    "        return inputs\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        #B_U = states[1]  # dropout matrices for recurrent units\n",
    "        #B_W = states[2]\n",
    "        h_tm1a = K.dot(h_tm1, self.Wa)\n",
    "        eij = K.dot(K.tanh(K.repeat(h_tm1a, K.shape(self.h)[1]) + self.ha), self.Va)\n",
    "        eijs = K.squeeze(eij, -1)\n",
    "        alphaij = K.softmax(eijs) # batchsize * lenh       h batchsize * lenh * ndim\n",
    "        ci = K.permute_dimensions(K.permute_dimensions(self.h, [2,0,1]) * alphaij, [1,2,0])\n",
    "        cisum = K.sum(ci, axis=1)\n",
    "        #print(K.shape(cisum), cisum.shape, ci.shape, self.h.shape, alphaij.shape, x.shape)\n",
    "\n",
    "        zr = K.sigmoid(K.dot(inputs, self.Wzr) + K.dot(h_tm1, self.Uzr) + K.dot(cisum, self.Czr))\n",
    "        zi = zr[:, :self.units]\n",
    "        ri = zr[:, self.units: 2 * self.units]\n",
    "        si_ = K.tanh(K.dot(inputs, self.W) + K.dot(ri*h_tm1, self.U) + K.dot(cisum, self.C))\n",
    "        si = (1-zi) * h_tm1 + zi * si_\n",
    "        return si, [si] #h_tm1, [h_tm1]\n",
    "        '''if self.consume_less == 'gpu':\n",
    "            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n",
    "            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.units])\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            inner_z = matrix_inner[:, :self.units]\n",
    "            inner_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "            z = self.inner_activation(x_z + inner_z)\n",
    "            r = self.inner_activation(x_r + inner_r)\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + inner_h)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_z = x[:, :self.units]\n",
    "                x_r = x[:, self.units: 2 * self.units]\n",
    "                x_h = x[:, 2 * self.units:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n",
    "                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n",
    "                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n",
    "            else:\n",
    "                raise ValueError('Unknown `consume_less` mode.')\n",
    "            z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n",
    "            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n",
    "            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        return h, [h]'''\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        '''if 0 < self.dropout_U < 1:\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n",
    "            constants.append(B_U)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        if 0 < self.dropout_W < 1:\n",
    "            input_shape = K.int_shape(x)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n",
    "            constants.append(B_W)\n",
    "        else:'''\n",
    "        constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        return constants\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer)}\n",
    "        base_config = super(SimpleAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SimpleAttention(Recurrent):\n",
    "    \"\"\"Attention Recurrent Unit - Bengio et al. ICLR 2015.\n",
    "    # Arguments\n",
    "        units: dimension of the internal projections and the final output.\n",
    "        h: we use it as attention to process the input\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, h, h_dim,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                 #activation='tanh', inner_activation='hard_sigmoid',\n",
    "                 #W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                 #dropout_W=0., dropout_U=0., \n",
    "                 **kwargs):\n",
    "        self.units = units\n",
    "        self.h = h\n",
    "        self.h_dim = h_dim\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        #self.activation = activations.get(activation)\n",
    "        #self.inner_activation = activations.get(inner_activation)\n",
    "        #self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        #self.U_regularizer = regularizers.get(U_regularizer)\n",
    "        #self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        #self.dropout_W = dropout_W\n",
    "        #self.dropout_U = dropout_U\n",
    "\n",
    "        #if self.dropout_W or self.dropout_U:\n",
    "        #    self.uses_learning_phase = True\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.input_dim = input_shape[2]\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: all-zero tensor of shape (units)\n",
    "            self.states = [None]\n",
    "\n",
    "        self.Wa = self.add_weight((self.units, self.units),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Wa'.format(self.name))\n",
    "        self.Ua = self.add_weight((self.h_dim, self.units),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Ua'.format(self.name))\n",
    "        self.Va = self.add_weight((self.units,1),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Va'.format(self.name))\n",
    "        self.Wzr = self.add_weight((self.input_dim, 2 * self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Uzr = self.add_weight((self.units, 2 * self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Czr = self.add_weight((self.h_dim, 2 * self.units),\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   name='{}_Czr'.format(self.name))\n",
    "        self.W = self.add_weight((self.input_dim, self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        self.U = self.add_weight((self.units, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_U'.format(self.name))\n",
    "        self.C = self.add_weight((self.h_dim, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_C'.format(self.name))\n",
    "        \n",
    "        #if self.initial_weights is not None:\n",
    "        #    self.set_weights(self.initial_weights)\n",
    "        #    del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        if not input_shape[0]:\n",
    "            raise ValueError('If a RNN is stateful, a complete '\n",
    "                             'input_shape must be provided '\n",
    "                             '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.units)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.units))]\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        #if self.consume_less == 'cpu':\n",
    "        #    input_shape = K.int_shape(x)\n",
    "        #    input_dim = input_shape[2]\n",
    "        #    timesteps = input_shape[1]\n",
    "\n",
    "        #    x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    return K.concatenate([x_z, x_r, x_h], axis=2)\n",
    "        #else:\n",
    "        #    return x\n",
    "        self.ha = K.dot(self.h, self.Ua) #time_distributed_dense(self.h, self.Ua)\n",
    "        return inputs\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        #B_U = states[1]  # dropout matrices for recurrent units\n",
    "        #B_W = states[2]\n",
    "        h_tm1a = K.dot(h_tm1, self.Wa)\n",
    "        eij = K.dot(K.tanh(h_tm1a + self.ha), self.Va)\n",
    "        eijs = K.repeat_elements(eij, self.h_dim, axis=1)\n",
    "\n",
    "        #alphaij = K.softmax(eijs) # batchsize * lenh       h batchsize * lenh * ndim\n",
    "        #ci = K.permute_dimensions(K.permute_dimensions(self.h, [2,0,1]) * alphaij, [1,2,0])\n",
    "        #cisum = K.sum(ci, axis=1)\n",
    "        cisum = eijs*self.h\n",
    "        #print(K.shape(cisum), cisum.shape, ci.shape, self.h.shape, alphaij.shape, x.shape)\n",
    "\n",
    "        zr = K.sigmoid(K.dot(inputs, self.Wzr) + K.dot(h_tm1, self.Uzr) + K.dot(cisum, self.Czr))\n",
    "        zi = zr[:, :self.units]\n",
    "        ri = zr[:, self.units: 2 * self.units]\n",
    "        si_ = K.tanh(K.dot(inputs, self.W) + K.dot(ri*h_tm1, self.U) + K.dot(cisum, self.C))\n",
    "        si = (1-zi) * h_tm1 + zi * si_\n",
    "        return si, [si] #h_tm1, [h_tm1]\n",
    "        '''if self.consume_less == 'gpu':\n",
    "            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n",
    "            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.units])\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            inner_z = matrix_inner[:, :self.units]\n",
    "            inner_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "            z = self.inner_activation(x_z + inner_z)\n",
    "            r = self.inner_activation(x_r + inner_r)\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + inner_h)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_z = x[:, :self.units]\n",
    "                x_r = x[:, self.units: 2 * self.units]\n",
    "                x_h = x[:, 2 * self.units:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n",
    "                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n",
    "                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n",
    "            else:\n",
    "                raise ValueError('Unknown `consume_less` mode.')\n",
    "            z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n",
    "            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n",
    "            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        return h, [h]'''\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        '''if 0 < self.dropout_U < 1:\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n",
    "            constants.append(B_U)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        if 0 < self.dropout_W < 1:\n",
    "            input_shape = K.int_shape(x)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n",
    "            constants.append(B_W)\n",
    "        else:'''\n",
    "        constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        return constants\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer)}\n",
    "        base_config = super(SimpleAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SSimpleAttention(Recurrent):\n",
    "    \"\"\"Attention Recurrent Unit - Bengio et al. ICLR 2015.\n",
    "    # Arguments\n",
    "        units: dimension of the internal projections and the final output.\n",
    "        h: we use it as attention to process the input\n",
    "        init: weight initialization function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [initializations](../initializations.md)).\n",
    "        inner_init: initialization function of the inner cells.\n",
    "        activation: activation function.\n",
    "            Can be the name of an existing function (str),\n",
    "            or a Theano function (see: [activations](../activations.md)).\n",
    "        inner_activation: activation function for the inner cells.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the input weights matrices.\n",
    "        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n",
    "        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the bias.\n",
    "        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, h, h_dim,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                 #activation='tanh', inner_activation='hard_sigmoid',\n",
    "                 #W_regularizer=None, U_regularizer=None, b_regularizer=None,\n",
    "                 #dropout_W=0., dropout_U=0., \n",
    "                 **kwargs):\n",
    "        self.units = units\n",
    "        self.h = h[:,-1,:]\n",
    "        self.h_dim = h_dim\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        #self.activation = activations.get(activation)\n",
    "        #self.inner_activation = activations.get(inner_activation)\n",
    "        #self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        #self.U_regularizer = regularizers.get(U_regularizer)\n",
    "        #self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        #self.dropout_W = dropout_W\n",
    "        #self.dropout_U = dropout_U\n",
    "\n",
    "        #if self.dropout_W or self.dropout_U:\n",
    "        #    self.uses_learning_phase = True\n",
    "        super(SSimpleAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.input_dim = input_shape[2]\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            # initial states: all-zero tensor of shape (units)\n",
    "            self.states = [None]\n",
    "\n",
    "        self.Wa = self.add_weight((self.units, self.units),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Wa'.format(self.name))\n",
    "        self.Ua = self.add_weight((self.h_dim, self.units),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Ua'.format(self.name))\n",
    "        self.Va = self.add_weight((self.units,1),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  name='{}_Va'.format(self.name))\n",
    "        self.Wzr = self.add_weight((self.input_dim, 2 * self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Uzr = self.add_weight((self.units, 2 * self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Czr = self.add_weight((self.h_dim, 2 * self.units),\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   name='{}_Czr'.format(self.name))\n",
    "        self.W = self.add_weight((self.input_dim, self.units),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        self.U = self.add_weight((self.units, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_U'.format(self.name))\n",
    "        self.C = self.add_weight((self.h_dim, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_C'.format(self.name))\n",
    "        \n",
    "        #if self.initial_weights is not None:\n",
    "        #    self.set_weights(self.initial_weights)\n",
    "        #    del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        if not input_shape[0]:\n",
    "            raise ValueError('If a RNN is stateful, a complete '\n",
    "                             'input_shape must be provided '\n",
    "                             '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.units)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.units))]\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        #if self.consume_less == 'cpu':\n",
    "        #    input_shape = K.int_shape(x)\n",
    "        #    input_dim = input_shape[2]\n",
    "        #    timesteps = input_shape[1]\n",
    "\n",
    "        #    x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,\n",
    "        #                                 input_dim, self.units, timesteps)\n",
    "        #    return K.concatenate([x_z, x_r, x_h], axis=2)\n",
    "        #else:\n",
    "        #    return x\n",
    "        self.ha = K.dot(self.h, self.Ua) #time_distributed_dense(self.h, self.Ua)\n",
    "        return inputs\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        #B_U = states[1]  # dropout matrices for recurrent units\n",
    "        #B_W = states[2]\n",
    "        h_tm1a = K.dot(h_tm1, self.Wa)\n",
    "        eij = K.dot(K.tanh(h_tm1a + self.ha), self.Va)\n",
    "        eijs = K.repeat_elements(eij, self.h_dim, axis=1)\n",
    "\n",
    "        #alphaij = K.softmax(eijs) # batchsize * lenh       h batchsize * lenh * ndim\n",
    "        #ci = K.permute_dimensions(K.permute_dimensions(self.h, [2,0,1]) * alphaij, [1,2,0])\n",
    "        #cisum = K.sum(ci, axis=1)\n",
    "        cisum = eijs*self.h\n",
    "        #print(K.shape(cisum), cisum.shape, ci.shape, self.h.shape, alphaij.shape, x.shape)\n",
    "\n",
    "        zr = K.sigmoid(K.dot(inputs, self.Wzr) + K.dot(h_tm1, self.Uzr) + K.dot(cisum, self.Czr))\n",
    "        zi = zr[:, :self.units]\n",
    "        ri = zr[:, self.units: 2 * self.units]\n",
    "        si_ = K.tanh(K.dot(inputs, self.W) + K.dot(ri*h_tm1, self.U) + K.dot(cisum, self.C))\n",
    "        si = (1-zi) * h_tm1 + zi * si_\n",
    "        return si, [si] #h_tm1, [h_tm1]\n",
    "        '''if self.consume_less == 'gpu':\n",
    "            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n",
    "            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.units])\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            inner_z = matrix_inner[:, :self.units]\n",
    "            inner_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "            z = self.inner_activation(x_z + inner_z)\n",
    "            r = self.inner_activation(x_r + inner_r)\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + inner_h)\n",
    "        else:\n",
    "            if self.consume_less == 'cpu':\n",
    "                x_z = x[:, :self.units]\n",
    "                x_r = x[:, self.units: 2 * self.units]\n",
    "                x_h = x[:, 2 * self.units:]\n",
    "            elif self.consume_less == 'mem':\n",
    "                x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n",
    "                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n",
    "                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n",
    "            else:\n",
    "                raise ValueError('Unknown `consume_less` mode.')\n",
    "            z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n",
    "            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n",
    "            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        return h, [h]'''\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        '''if 0 < self.dropout_U < 1:\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "            B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n",
    "            constants.append(B_U)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        if 0 < self.dropout_W < 1:\n",
    "            input_shape = K.int_shape(x)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "            B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n",
    "            constants.append(B_W)\n",
    "        else:'''\n",
    "        constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        return constants\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer)}\n",
    "        base_config = super(SSimpleAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SimpleAttention2(Recurrent):\n",
    "    def __init__(self, units, h_dim,\n",
    "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                 **kwargs):\n",
    "        self.units = units\n",
    "        self.h_dim = h_dim\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        super(SimpleAttention2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.input_dim = input_shape[2] - self.h_dim\n",
    "\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            self.states = [None]\n",
    "\n",
    "        self.Wa = self.add_weight((self.units, self.units),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Wa'.format(self.name))\n",
    "        self.Ua = self.add_weight((self.h_dim, self.units),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Ua'.format(self.name))\n",
    "        self.Va = self.add_weight((self.units,1),\n",
    "                                  initializer=self.recurrent_initializer,\n",
    "                                  name='{}_Va'.format(self.name))\n",
    "        self.Wzr = self.add_weight((self.input_dim, 2 * self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Uzr = self.add_weight((self.units, 2 * self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_Wzr'.format(self.name))\n",
    "        self.Czr = self.add_weight((self.h_dim, 2 * self.units),\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   name='{}_Czr'.format(self.name))\n",
    "        self.W = self.add_weight((self.input_dim, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        self.U = self.add_weight((self.units, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_U'.format(self.name))\n",
    "        self.C = self.add_weight((self.h_dim, self.units),\n",
    "                                 initializer=self.recurrent_initializer,\n",
    "                                 name='{}_C'.format(self.name))\n",
    "        self.built = True\n",
    "\n",
    "    def reset_states(self):\n",
    "        assert self.stateful, 'Layer must be stateful.'\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        if not input_shape[0]:\n",
    "            raise ValueError('If a RNN is stateful, a complete '\n",
    "                             'input_shape must be provided '\n",
    "                             '(including batch size).')\n",
    "        if hasattr(self, 'states'):\n",
    "            K.set_value(self.states[0],\n",
    "                        np.zeros((input_shape[0], self.units)))\n",
    "        else:\n",
    "            self.states = [K.zeros((input_shape[0], self.units))]\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        #self.ha = K.dot(self.h, self.Ua) #time_distributed_dense(self.h, self.Ua)\n",
    "        return inputs\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        #B_U = states[1]  # dropout matrices for recurrent units\n",
    "        #B_W = states[2]\n",
    "        h_tm1a = K.dot(h_tm1, self.Wa)\n",
    "        eij = K.dot(K.tanh(h_tm1a + K.dot(inputs[:, :self.h_dim], self.Ua)), self.Va)\n",
    "        eijs = K.repeat_elements(eij, self.h_dim, axis=1)\n",
    "\n",
    "        #alphaij = K.softmax(eijs) # batchsize * lenh       h batchsize * lenh * ndim\n",
    "        #ci = K.permute_dimensions(K.permute_dimensions(self.h, [2,0,1]) * alphaij, [1,2,0])\n",
    "        #cisum = K.sum(ci, axis=1)\n",
    "        cisum = eijs*inputs[:, :self.h_dim]\n",
    "        #print(K.shape(cisum), cisum.shape, ci.shape, self.h.shape, alphaij.shape, x.shape)\n",
    "\n",
    "        zr = K.sigmoid(K.dot(inputs[:, self.h_dim:], self.Wzr) + K.dot(h_tm1, self.Uzr) + K.dot(cisum, self.Czr))\n",
    "        zi = zr[:, :self.units]\n",
    "        ri = zr[:, self.units: 2 * self.units]\n",
    "        si_ = K.tanh(K.dot(inputs[:, self.h_dim:], self.W) + K.dot(ri*h_tm1, self.U) + K.dot(cisum, self.C))\n",
    "        si = (1-zi) * h_tm1 + zi * si_\n",
    "        return si, [si] #h_tm1, [h_tm1]\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
    "        return constants\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer)}\n",
    "        base_config = super(SimpleAttention2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
