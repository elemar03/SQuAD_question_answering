{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SQuAD Data Preprocessing for OWQA\n",
    "---\n",
    "\n",
    "Inspired from the Stanford's course \"CS224N\" - winter 2018 SQuAD preprocessing on <a href=\"https://github.com/abisee/cs224n-win18-squad/blob/master/code/preprocessing/squad_preprocess.py\">GitHub</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def import_json(filename):\n",
    "    \"\"\"load json from a file filename and returns it\"\"\"\n",
    "    with open(\"../../data/\" + filename) as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def get_data_from_file(tier):\n",
    "    \"\"\"retrieve the data dictionary from the preprocessed files\"\"\"\n",
    "    contexts = np.load(\"../../data/ow_preprocessed/\"+tier+\"_contexts\"+\".npy\", allow_pickle=True)\n",
    "    questions = np.load(\"../../data/ow_preprocessed/\"+tier+\"_questions\"+\".npy\", allow_pickle=True)\n",
    "    contexts_tokens = np.load(\"../../data/ow_preprocessed/\"+tier+\"_contexts_tokens\"+\".npy\", allow_pickle=True)\n",
    "    questions_tokens = np.load(\"../../data/ow_preprocessed/\"+tier+\"_questions_tokens\"+\".npy\", allow_pickle=True)\n",
    "    answers = np.load(\"../../data/ow_preprocessed/\"+tier+\"_answers\"+\".npy\", allow_pickle=True)\n",
    "    start_charloc = np.load(\"../../data/ow_preprocessed/\"+tier+\"_start_charloc\"+\".npy\", allow_pickle=True)\n",
    "    end_charloc = np.load(\"../../data/ow_preprocessed/\"+tier+\"_end_charloc\"+\".npy\", allow_pickle=True)\n",
    "    start_wordloc = np.load(\"../../data/ow_preprocessed/\"+tier+\"_start_wordloc\"+\".npy\", allow_pickle=True)\n",
    "    end_wordloc = np.load(\"../../data/ow_preprocessed/\"+tier+\"_end_wordloc\"+\".npy\", allow_pickle=True)\n",
    "    \n",
    "    data = {\"contexts\": contexts,\n",
    "            \"questions\": questions,\n",
    "            \"contexts_tokens\": contexts_tokens,\n",
    "            \"questions_tokens\": questions_tokens,\n",
    "            \"answers\": answers,\n",
    "            \"start_charloc\": start_charloc,\n",
    "            \"end_charloc\": end_charloc,\n",
    "            \"start_wordloc\": start_wordloc,\n",
    "            \"end_wordloc\": end_wordloc\n",
    "           }\n",
    "    return data\n",
    "\n",
    "    \n",
    "def write_data(tier, data):\n",
    "    print(\"** Writing the preprocessed data in the respective files. . .\")\n",
    "    \n",
    "    # write tokenized data to file\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_contexts\", data['contexts'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_questions\", data['questions'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_contexts_tokens\", data['contexts_tokens'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_questions_tokens\", data['questions_tokens'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_answers\", data['answers'])\n",
    "\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_start_charloc\", data['start_charloc'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_end_charloc\", data['end_charloc'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_start_wordloc\", data['start_wordloc'])\n",
    "    np.save(\"../../data/ow_preprocessed/\"+tier+\"_end_wordloc\", data['end_wordloc'])\n",
    "\n",
    "\n",
    "def max_len(sequences):\n",
    "    \"\"\"Calculate the maximum length of the sequences in a list\"\"\"\n",
    "    _max = 0;\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) > _max:\n",
    "            _max = len(sequence)\n",
    "    return _max\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"The following replacements:\n",
    "    token.replace(\"``\", '\"'), token.replace(\"''\", '\"')\n",
    "    are suggested in the paper BidAF (Seo et al., 2016)\"\"\"\n",
    "    text = [[token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in word_tokenize(sequence)] for sequence in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "def check_indices(data):\n",
    "    \"\"\"Checks if the mapping from character location of the answer\n",
    "    to word/token location of the answer is correct for every sample.\n",
    "    Returns the indices of unsuccessful mapping.\"\"\"\n",
    "    wrong_tokens = []\n",
    "    for i in range(len(data['contexts'])):\n",
    "        answer = data['contexts'][i][data['start_charloc'][i]:data['end_charloc'][i]]\n",
    "        a_tokens = data['contexts_tokens'][i][data['start_wordloc'][i]:data['end_wordloc'][i]]\n",
    "    \n",
    "        if \"\".join(a_tokens) != \"\".join(answer.split()):\n",
    "            wrong_tokens.append(i)\n",
    "    print(\"Not successful char loc -> word loc mapping: \", len(wrong_tokens))\n",
    "    return wrong_tokens\n",
    "\n",
    "\n",
    "def find_one_word_answ(token_starts, token_ends):\n",
    "    \"\"\"Finds the indices of all the not-one word answers\"\"\"\n",
    "    not_one_word_indices = []\n",
    "    for i in range(len(token_starts)):\n",
    "        if token_starts[i] != token_ends[i]-1:\n",
    "            not_one_word_indices.append(i)\n",
    "            \n",
    "    print(\"Number of not one-word answers: \", len(not_one_word_indices))\n",
    "    return not_one_word_indices\n",
    "\n",
    "\n",
    "def remove_entries(indices, data):\n",
    "    \"\"\"Removes entries of particular indices in all the lists\"\"\"\n",
    "    print('Dataset length before: ', len(data['contexts']))\n",
    "    \n",
    "    for i in range(len(indices)-1, -1, -1):\n",
    "        del data['contexts'][indices[i]]\n",
    "        del data['questions'][indices[i]]\n",
    "        del data['contexts_tokens'][indices[i]]\n",
    "        del data['questions_tokens'][indices[i]]\n",
    "        del data['answers'][indices[i]]\n",
    "        del data['start_charloc'][indices[i]]\n",
    "        del data['end_charloc'][indices[i]]\n",
    "        del data['start_wordloc'][indices[i]]\n",
    "        del data['end_wordloc'][indices[i]]\n",
    "    \n",
    "    print('Dataset length after: ', len(data['contexts']))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Importing the Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(json_file, contexts_max_len=None, max_size=None):\n",
    "    num_spanalign_prob = 0\n",
    "    \n",
    "    json_data = import_json(json_file)\n",
    "    \n",
    "    if contexts_max_len is not None:\n",
    "        print('** Considering only the contexts smaller than ', contexts_max_len, ' . . .')\n",
    "    \n",
    "    data = {\"contexts\": [],         # contexts/paragraphs\n",
    "            \"questions\": [],        # questions\n",
    "            \"contexts_tokens\": [],  # contexts tokens\n",
    "            \"questions_tokens\": [], # questions tokens\n",
    "            \"answers\": [],          # answers\n",
    "            \"start_charloc\": [],    # character indices indicating the begin of the answer\n",
    "            \"end_charloc\": [],      # character indices indicating the end of the answer\n",
    "            \"start_wordloc\": [],    # word token indices indicating the begin of the answer\n",
    "            \"end_wordloc\": []       # word token indices indicating the end of the answer\n",
    "           }\n",
    "    \n",
    "    for dt in json_data['data']:\n",
    "        paragraphs = dt['paragraphs']\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            context = str(paragraph['context'])\n",
    "            context = context.lower()\n",
    "            \n",
    "            if contexts_max_len is not None:\n",
    "                if len(context) > contexts_max_len:\n",
    "                    continue\n",
    "    \n",
    "            qas = paragraph['qas'] #  list of questions\n",
    "            for qa in qas:\n",
    "                question = str(qa['question'])\n",
    "                question = question.lower()\n",
    "                answer = qa['answers'][0]\n",
    "                answer_text = str(answer['text']).lower() # get the answer text\n",
    "                start_charloc = answer['answer_start'] # answer start loc (character count)\n",
    "                end_charloc = start_charloc + len(answer_text) # answer end loc (character count)\n",
    "\n",
    "                # Check that the provided character spans match the provided answer text\n",
    "                # Sometimes this is misaligned, mostly because Python can interpret\n",
    "                # certain Unicode characters to have length 2\n",
    "                # https://stackoverflow.com/questions/29109944/\n",
    "                # python-returns-length-of-2-for-single-unicode-character-string  \n",
    "                if answer_text != context[start_charloc : end_charloc]:\n",
    "                    num_spanalign_prob += 1\n",
    "                    continue\n",
    "                \n",
    "                data['contexts'].append(context)\n",
    "                data['questions'].append(question)\n",
    "                data['answers'].append(answer_text)\n",
    "                data['start_charloc'].append(start_charloc)\n",
    "                data['end_charloc'].append(end_charloc)\n",
    "                \n",
    "    if max_size is not None:\n",
    "        print('** Reducing the size of the dataset to ', max_size, ' . . .')\n",
    "        data['contexts'] = data['contexts'][:max_size]\n",
    "        data['questions'] = data['questions'][:max_size]\n",
    "        data['answers'] = data['answers'][:max_size]\n",
    "        data['start_charloc'] = data['start_charloc'][:max_size]\n",
    "        data['end_charloc'] = data['end_charloc'][:max_size]\n",
    "        \n",
    "    # check they have all the same length\n",
    "    assert len(data['contexts'])==len(data['questions'])==len(data['answers'])\n",
    "    assert len(data['answers'])==len(data['start_charloc'])==len(data['end_charloc'])\n",
    "    print(\"Number of span align problems: \", num_spanalign_prob)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mapping Character Indices to Token Indices\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_char_to_token_index(data):\n",
    "    \"\"\"Map character location of the answer\n",
    "    to word/token location of the answer\"\"\"\n",
    "    print('** Mapping character location to word location of the answer . . .')\n",
    "    word_starts = []  # list to store the token start locations\n",
    "    word_ends = []  # list to store the token end locations\n",
    "    \n",
    "    for i, context in enumerate(data['contexts']):\n",
    "        \"\"\"loop through the contexts.\n",
    "        for every single context and context tokens,\n",
    "        take note of the current character index of the context, \n",
    "        the character index of the token and\n",
    "        the index of the current token word of the context we are in\"\"\"\n",
    "        tokens = data['contexts_tokens'][i] # single context's tokens\n",
    "        char_i = 0 # Character index of the context\n",
    "        token_i = 0 # Character index of the token\n",
    "        current_token = 0 # Index of the current token word of the context we are in\n",
    "        \n",
    "        while char_i < len(context): # Loop through the characters in the context\n",
    "            if current_token < len(tokens):\n",
    "                context_char = context[char_i]  # current char in the context\n",
    "                token_char = tokens[current_token][token_i]  # current char in the tokens\n",
    "\n",
    "                if data['start_charloc'][i] == char_i:\n",
    "                    word_starts.append(current_token)\n",
    "\n",
    "                if data['end_charloc'][i] == char_i:\n",
    "                    word_ends.append(current_token)\n",
    "\n",
    "                \"\"\" skip the space, the only character that the contexts and\n",
    "                the context tokens don't have in common.\n",
    "                if token_char != context_char, it means that we reached a space.\n",
    "                therefore we need to increase the token_i and consider the next token.\"\"\"\n",
    "                if token_char == context_char:\n",
    "                    if token_i == len(tokens[current_token])-1:\n",
    "                        current_token += 1\n",
    "                        token_i = 0\n",
    "                    else:\n",
    "                        token_i += 1\n",
    "            char_i += 1\n",
    "            \n",
    "        if len(word_ends) <= i:\n",
    "            word_ends.append(current_token)\n",
    "        \n",
    "    return word_starts, word_ends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Text Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, char_level=False, filter_one_word_answ=True):\n",
    "    \n",
    "    print(\"** Tokenizing contexts and questions. . .\")\n",
    "    data['contexts_tokens'] = tokenize(data['contexts'])\n",
    "    data['questions_tokens'] = tokenize(data['questions'])\n",
    "    \n",
    "    data[\"start_wordloc\"], data[\"end_wordloc\"] = map_char_to_token_index(data)\n",
    "    wrong_indices = check_indices(data)\n",
    "        \n",
    "    if len(wrong_indices) > 0:\n",
    "        print('** Discarding', len(wrong_indices), 'entries due to char loc -> word loc mapping problems . . .')\n",
    "        \n",
    "        data = remove_entries(wrong_indices, data)\n",
    "        wrong_indices = check_indices(data)\n",
    "        \n",
    "    if filter_one_word_answ:\n",
    "        not_one_word_indices = find_one_word_answ(data[\"start_wordloc\"], data[\"end_wordloc\"])\n",
    "\n",
    "        if len(not_one_word_indices) > 0:\n",
    "            print('** Filtering One-Word Answers only. . .')\n",
    "            \n",
    "            data = remove_entries(not_one_word_indices, data)\n",
    "            not_one_word_indices = find_one_word_answ(data[\"start_wordloc\"], data[\"end_wordloc\"])\n",
    "            \n",
    "    if char_level:\n",
    "        data['contexts_tokens'] = data['contexts']\n",
    "        data['questions_tokens'] = data['questions']\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def onehot_labels(starts, ends, c_maxlen):\n",
    "    \n",
    "    print('** One-hot encoding the labels in two tensors . . .')\n",
    "    labels_start = np.zeros( (len(starts), c_maxlen) )\n",
    "    labels_end = np.zeros( (len(ends), c_maxlen) )\n",
    "        \n",
    "    for i in range(len(starts)):\n",
    "        labels_start[i, starts[i]] = 1\n",
    "        labels_end[i, ends[i]] = 1\n",
    "\n",
    "    return labels_start, labels_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Main function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_preprocessing_main(contexts_max_len=None, max_size=None):\n",
    "\n",
    "    train_data = import_dataset(\"squad_train.json\", contexts_max_len=contexts_max_len, max_size=max_size)\n",
    "    print(\"Train data length = %i\" % len(train_data['contexts']))\n",
    "    dev_data = import_dataset(\"squad_dev.json\", contexts_max_len=contexts_max_len, max_size=max_size)\n",
    "    print(\"Test data length = %i\" % len(dev_data['contexts']))\n",
    "    \n",
    "    char_level = False\n",
    "    print(\"\\n - train -\")\n",
    "    train_data = preprocess(train_data, char_level=char_level, filter_one_word_answ=True)\n",
    "    print(\"\\n - dev -\")\n",
    "    dev_data = preprocess(dev_data, char_level=char_level, filter_one_word_answ=True)\n",
    "    \n",
    "    t = Tokenizer(filters='', char_level=char_level)\n",
    "\n",
    "    t.fit_on_texts(train_data['contexts_tokens'])\n",
    "    t.fit_on_texts(train_data['questions_tokens'])\n",
    "    t.fit_on_texts(dev_data['contexts_tokens'])\n",
    "    t.fit_on_texts(dev_data['questions_tokens'])\n",
    "\n",
    "    vocabulary_size = len(t.word_index)\n",
    "    print('Vocabulary size: ', vocabulary_size)\n",
    "    print('** Converting the tokens into sequences . . .')\n",
    "    train_data['contexts'] = t.texts_to_sequences(train_data['contexts_tokens'])\n",
    "    train_data['questions'] = t.texts_to_sequences(train_data['questions_tokens'])\n",
    "    dev_data['contexts'] = t.texts_to_sequences(dev_data['contexts_tokens'])\n",
    "    dev_data['questions'] = t.texts_to_sequences(dev_data['questions_tokens'])\n",
    "\n",
    "    # Calculate the maximum length of the contexts and the questions\n",
    "    train_cmaxlen = np.max(train_data['end_wordloc'])+1\n",
    "    dev_cmaxlen = np.max(dev_data['end_wordloc'])+1\n",
    "    \n",
    "    train_qmaxlen = max_len(train_data['questions'])\n",
    "    dev_qmaxlen = max_len(dev_data['questions'])\n",
    "    \n",
    "    c_maxlen = max(train_cmaxlen, dev_cmaxlen)\n",
    "    q_maxlen = max(train_qmaxlen, dev_qmaxlen)\n",
    "    \n",
    "    print(' -- Ends max length train/dev: ', train_cmaxlen-1, dev_cmaxlen-1)\n",
    "    print(' -- Contexts max length: ', c_maxlen)\n",
    "    print(' -- Questions max length: ', q_maxlen)\n",
    "    \n",
    "    print('** Padding the sequences . . .')\n",
    "    train_data['contexts'] = pad_sequences(train_data['contexts'],\n",
    "                                           maxlen=c_maxlen, padding='post', truncating='post')\n",
    "    train_data['questions'] = pad_sequences(train_data['questions'],\n",
    "                                            maxlen=q_maxlen, padding='post')\n",
    "    dev_data['contexts'] = pad_sequences(dev_data['contexts'],\n",
    "                                         maxlen=c_maxlen, padding='post', truncating='post')\n",
    "    dev_data['questions'] = pad_sequences(dev_data['questions'],\n",
    "                                          maxlen=q_maxlen, padding='post')\n",
    "    \n",
    "    assert max_len(train_data['contexts']) == max_len(dev_data['contexts'])\n",
    "    assert max_len(train_data['questions']) == max_len(dev_data['questions'])\n",
    "    \n",
    "    for i in range(len(train_data['contexts'])):\n",
    "        assert(len(train_data['contexts'][i]) >= np.max(train_data[\"end_wordloc\"])+1)\n",
    "        assert(train_data['start_wordloc'][i] == train_data['end_wordloc'][i]-1)\n",
    "    for i in range(len(dev_data['contexts'])):\n",
    "        assert(len(dev_data['contexts'][i]) >= np.max(dev_data[\"end_wordloc\"])+1)\n",
    "        assert(dev_data['start_wordloc'][i] == dev_data['end_wordloc'][i]-1)\n",
    "    \n",
    "    # write to file: context, question tensors, answers, numbers for char/word start/end locations.\n",
    "    write_data(\"train\", train_data)\n",
    "    write_data(\"dev\", dev_data)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
